---
title: 决策树
date: 2019-09-15 21:28:43
tags: 
- 数据挖掘
- 机器学习
- sklearn
categories:
- 机器学习
---

决策树(Decision Tree)是一种非参数的有监督学习方法, 它能够从一系列有特征和标签的数据中总结出决策规则, 并用树状图的结构来展现这些规则, 以解决分类和回归问题.

<!-- more -->

**决策树算法的核心是要解决两个问题:**

1. 如何从数据表中找出最佳节点和最佳分支?
2. 如何让决策树停止生长,防止过拟合?

# sklearn中的决策树

sklearn中的决策树相关的类都在"tree"这个模块下,这个模块总共包含五个类:

| tree.DecisionTreeClassifier | 分类树                                |
| --------------------------- | ------------------------------------- |
| tree.DecisionTreeRegressor  | 回归树                                |
| tree.export_graphviz        | 将生成的决策树导出为DOT格式，画图专用 |
| tree.ExtraTreeClassifier    | 高随机版本的分类树                    |
| tree.ExtraTreeRegressor     | 高随机版本的回归树                    |
|                             |                                       |


# 欠拟合与过拟合

## 欠拟合

- 欠拟合指的是模型没有很好地捕捉到数据特征，不能够很好地拟合数据.
- 解决方法:

1. 添加其他特征项,有时候模型出现欠拟合的时候是因为特征项不够导致的,可以添加其他特征项来很好地解决.
2. 添加多项式特征,例如将线性模型通过添加二次项或者三次项使模型泛化能力更强.
3. 减少正则化参数,正则化的目的是用来防止过拟合的,但是现在模型出现了欠拟合,则需要减少正则化参数.

## 过拟合

- 过拟合指的是:模型训练的结果"太好了",以至于在实际应用的过程中,会存在"死板"的情况,导致分类错误.
- 造成过拟合的原因:

1. 训练集中样本量较小.如果决策树选择的属性过多,构造出来的决策树一定能够“完美”地把训练集中的样本分类,但是这样就会把训练集中一些数据的特点当成所有数据的特点,但这个特点不一定是全部数据的特点,这就使得这个决策树在真实的数据分类中出现错误,也就是模型的“泛化能力”差.

- 解决方法:
  
1. 重新清洗数据,导致过拟合的一个原因有可能是数据不纯导致的,如果出现了过拟合就需要重新清洗数据.
2. 增大数据的训练量,还有一个原因就是用于训练的数据量太小导致的,训练数据占总数据的比例过小.
3. 采用正则化方法.正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对应的范数,但是在机器学习中一般使用L2正则,下面看具体的原因.

- L0范数指的是:向量中非0的元素的个数。L1范数指的是:向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。两者都可以实现稀疏性，既然L0可以实现稀疏，为什么不用L0，而要用L1呢？个人理解一是因为L0范数很难优化求解（NP难问题），二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光和万千宠爱转于L1范数。

- L2范数是指向量各元素的平方和然后求平方根。可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。L2正则项起到使得参数w变小加剧的效果，但是为什么可以防止过拟合呢？一个通俗的理解便是：更小的参数值w意味着模型的复杂度更低，对训练数据的拟合刚刚好（奥卡姆剃刀），不会过分拟合训练数据，从而使得不会过拟合，以提高模型的泛化能力。还有就是看到有人说L2范数有助于处理 condition number不好的情况下矩阵求逆很困难的问题。

4. 采用dropout方法。这个方法在神经网络里面很常用。dropout方法是ImageNet中提出的一种方法，通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作。

# 决策树剪枝

## 预剪枝

在决策树构造时就进行剪枝.方法是在构造的过程中对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。

## 后剪枝

在生成决策树之后再进行剪枝，通常会从决策树的叶节点开始，逐层向上对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

## sklearn中的剪枝参数

- max_depth: 限制树的最大深度，超过设定深度的树枝全部剪掉。
- min_samples_leaf: 限定一个节点在分枝后的每个子节点都必须包含至少min_samples_leaf个训练样本，否则分枝就不会发生，或者，分枝会朝着满足每个子节点都包含min_samples_leaf个样本的方向去发生，一般搭配max_depth使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引起过拟合，设置得太大就会阻止模型学习数据。如果叶节点中含有的样本量变化很大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，等于1通常就是最佳选择。
- min_samples_split: 限定一个节点必须要包含至少min_samples_split个训练样本，这个节点才允许被分枝，否则分枝就不会发生。
- max_features: 限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和max_depth异曲同工， max_features是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量 而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型 学习不足。如果希望通过降维的方式防止过拟合，建议使用PCA，ICA或者特征选择模块中的降维算法。
- min_impurity_decrease: 限制信息增益的大小，信息增益小于设定数值的分枝不会发生。
- class_weight: 完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要 判断“一个办了信用卡的人是否会违约”，就是是vs否（1%：99%）的比例。这种分类状况下，即便模型什么也不 做，全把结果预测成“否”，正确率也能有99%。因此我们要使用class_weight参数对样本标签进行一定的均衡，给 少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认None，此模式表示自动给 与数据集中的所有标签相同的权重。
- min_weight_fraction_leaf: 有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配min_ weight_fraction_leaf这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如min_weight_ fraction_leaf）将比不知道样本权重的标准（比如min_samples_leaf）更偏向主导类。如果样本是加权的，则使用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。

